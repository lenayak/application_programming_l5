{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l4ha3JQ-kYJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Необходимо загрузить исходный набор данных и соответствующие метки классов.\n",
        "    Произвести разделение загруженного набора данных на обучающую, тестовую и валидационую выборки (в соотношении 80:10:10). Проверить, что сформированные выборки сбалансированы.\n",
        "    Написать модель нейронной сети для решения задачи классификации.\n",
        "    Описать пайплайн предобработки данных. ВАЖНО: что так как ваш вариант предполагает работу с текстом, то необходимо выполниить векторизацию данных (подробности в туториале).\n",
        "    Написать train loop (цикл обучения). Провести эксперименты по обучению с различными значениями параметров learning rate (скорость обучения) и batch size (размер мини-пакета). Выбрать по 3 значения для learning rate и batch size (итоговое количество экспериментов будет 9).\n",
        "    Для каждого проведенного эксперимента вывести графики для значения функции потерь (ось x - итерация обучения/номер эпохи; ось y - значение функции потерь) и выбранной метрики качества (ось x - итерация обучения/номер эпохи; ось y - значение метрики качества). Графики необходимо выводить как для обучающей, так и для валидационной выборки.\n",
        "    Оценить качество работы модели на тестовой выборке.\n",
        "    Сделайте выводы по полученным результатам проведенных экспериментов. Какую модель из всех полученных стоит использовать?\n",
        "    Сохранить обученную модель.\n",
        "    Выполните повторную инициализацию модели и загрузку весов. Продемонстрируйте работоспособность модели (пропустите через нее какой-то отзыв/рецензию и выведите результат).\n",
        "\n"
      ],
      "metadata": {
        "id": "29sniUpMkgy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from numpy import loadtxt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "from tqdm import trange\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import PorterStemmer\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n"
      ],
      "metadata": {
        "id": "lANiXnZzkjCD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHAbG8Mwpwom",
        "outputId": "ba89587b-afab-4391-a86d-520d4bfcade2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Необходимо загрузить исходный набор данных и соответствующие метки классов.\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "5zH-v1Y0k1RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Описать пайплайн предобработки данных. ВАЖНО: что так как ваш вариант предполагает работу с текстом, то необходимо выполниить векторизацию данных (подробности в туториале).\n",
        "data = pd.read_csv(\"dataframe.csv\")\n",
        "data.dropna(inplace=True)\n",
        "remove_non_alphabets = lambda x: re.sub(r'[^а-яА-Я]',' ',str(x))\n",
        "tokenize = lambda x: word_tokenize(x, language = \"russian\")\n",
        "\n",
        "\n",
        "def Lemmatize(src: list):\n",
        "    '''Lemmatizes the passed dataset'''\n",
        "    text_nomalized = ' '.join(src).lower() \n",
        "    m = Mystem()\n",
        "    lemmas = m.lemmatize(text_nomalized)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "print(\"remove_non_alphabets\")\n",
        "data['review'] = data[\"review\"].apply(remove_non_alphabets)\n",
        "print(\"tokenize\")\n",
        "data[\"review\"] = data[\"review\"].apply(tokenize)\n",
        "print(\"lemmatize\")\n",
        "for i in data.index:\n",
        "    data[\"review\"][i] = Lemmatize(data[\"review\"][i])\n",
        "print(\"complete\")\n",
        "data[\"review\"] = data[\"review\"].apply(lambda x: ' '.join(x))\n",
        "data.dropna(inplace=True)\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3oCZhnOlil9",
        "outputId": "ab2b6d52-3af3-4efe-8baa-246e9563b1bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove_non_alphabets\n",
            "tokenize\n",
            "lemmatize\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n",
            "<ipython-input-6-5566d0aae6a2>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[\"review\"][i] = Lemmatize(data['review'][i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete\n",
            "      mark                                             review\n",
            "0      1.0  однажды   вечер   вечер   вечеромпересматриват...\n",
            "1      1.0  на   фон   уносить   ветер   который   я   чит...\n",
            "2      1.0  ну   по   сравнение   с   этот   книга   тот  ...\n",
            "3      1.0  уже   очень   давно   следить   за   потуга   ...\n",
            "4      1.0  тропа   у   он   одинаковый   летний   ребенок...\n",
            "...    ...                                                ...\n",
            "4995   5.0  какой   удивительный   работа   проделывать   ...\n",
            "4996   5.0  в   очередной   раз   мочь   написать   сво   ...\n",
            "4997   5.0  смешной   история   о   то   как   два   жена ...\n",
            "4998   5.0  небольшой   занимательный   рассказ   этакий  ...\n",
            "4999   5.0  как   жаль   что   с   скляренко   не   успева...\n",
            "\n",
            "[4998 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Произвести разделение загруженного набора данных на обучающую, тестовую и валидационую выборки (в соотношении 80:10:10). Проверить, что сформированные выборки сбалансированы.\n",
        "frac_seed = random.randint(0,10)\n",
        "train_df = data.sample(frac=0.9, random_state=frac_seed, ignore_index=True)\n",
        "valid_df = data.sample(frac=0.1, random_state=frac_seed, ignore_index=True)\n",
        "for item in valid_df[\"review\"]:\n",
        "    while item in train_df['review']: \n",
        "        tmp = data.sample()\n",
        "        item[\"review\"] = tmp['review']\n",
        "        item[\"mark\"] = tmp[\"mark\"]\n",
        "    \n",
        "print(data.shape)\n",
        "print(train_df.shape)\n",
        "print(valid_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhopiM1Y6jy-",
        "outputId": "ebca6fd8-e32c-45be-b45a-3adf5e048843"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4998, 2)\n",
            "(4498, 2)\n",
            "(500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear1 = nn.Linear(10000, 100)\n",
        "        self.linear2 = nn.Linear(100, 10)\n",
        "        self.linear3 = nn.Linear(10, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "x_train = Variable(torch.from_numpy(x_train)).float()\n",
        "y_train = Variable(torch.from_numpy(y_train)).long()\n"
      ],
      "metadata": {
        "id": "pTO8gpkv8nop"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}